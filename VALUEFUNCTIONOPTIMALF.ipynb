{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjxJ3J5xQI3U"
      },
      "outputs": [],
      "source": [
        "### Code cell 0 ###\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(precision=3,suppress=True)   # print 3-decimal, no scientific\n",
        "\n",
        "env=gym.make('FrozenLake-v1',is_slippery=True)   # 4x4 frozen lake, stochastic\n",
        "obs,info=env.reset()\n",
        "\n",
        "print(\"Initial State:\",obs)\n",
        "print(\"Observation space size:\",env.observation_space.n)   # 16 states\n",
        "print(\"Action space size:\",env.action_space.n)             # 4 actions\n",
        "\n",
        "P=env.unwrapped.P    # transition dict: P[s][a] -> list of (prob,next_state,reward,done)\n",
        "\n",
        "# Reward model: print step, hole, goal rewards\n",
        "all_rewards={r for s in P for a in P[s] for(prob,next_state,r,done) in P[s][a]}\n",
        "print(\"Unique rewards in FrozenLake:\",all_rewards)\n",
        "\n",
        "reward_min=min(all_rewards)\n",
        "reward_max=max(all_rewards)\n",
        "print(\"Reward range:\",(reward_min,reward_max))\n",
        "\n",
        "# Value Iteration\n",
        "def value_iteration(env,discount_factor=0.99,theta=1e-6,max_iterations=10000):\n",
        "    nS=env.observation_space.n\n",
        "    nA=env.action_space.n\n",
        "    P=env.unwrapped.P\n",
        "    V=np.zeros(nS)          # V(s) init to 0 for all states\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        delta=0             # track max change in this sweep\n",
        "        for s in range(nS):\n",
        "            q_sa=np.zeros(nA)   # store Q(s,a) for all actions\n",
        "            for a in range(nA):\n",
        "                for prob,next_state,reward,done in P[s][a]:\n",
        "                    q_sa[a]+=prob*(reward+discount_factor*V[next_state])  # Bellman optimality\n",
        "            new_v=np.max(q_sa)          # V_new(s)=max_a Q(s,a)\n",
        "            delta=max(delta,abs(new_v-V[s]))\n",
        "            V[s]=new_v\n",
        "        if delta<theta:                 # stop when values almost stop changing\n",
        "            break\n",
        "\n",
        "    policy=extract_policy_from_v(env,V,discount_factor)  # greedy policy from V*\n",
        "    return V,policy,i+1\n",
        "\n",
        "# Extract policy\n",
        "def extract_policy_from_v(env,V,discount_factor=0.99):\n",
        "    nS=env.observation_space.n\n",
        "    nA=env.action_space.n\n",
        "    P=env.unwrapped.P\n",
        "    policy=np.zeros((nS,nA))   # each row: prob over 4 actions\n",
        "\n",
        "    for s in range(nS):\n",
        "        q_sa=np.zeros(nA)\n",
        "        for a in range(nA):\n",
        "            for prob,next_state,reward,done in P[s][a]:\n",
        "                q_sa[a]+=prob*(reward+discount_factor*V[next_state])\n",
        "        best=np.argmax(q_sa)               # best action index\n",
        "        policy[s]=np.eye(nA)[best]         # one-hot row\n",
        "    return policy\n",
        "\n",
        "# Plot V\n",
        "def plot_values(env,V,gamma_label=\"\"):\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.plot(V)                            # V over state index 0..15\n",
        "    title=\"Value Function (FrozenLake-v1)\"\n",
        "    if gamma_label:\n",
        "        title+=\" (gamma=\"+gamma_label+\")\"\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"State (0â€“15)\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot policy\n",
        "def plot_policy(env,policy,gamma_label=\"\"):\n",
        "    nS=env.observation_space.n\n",
        "    actions=np.argmax(policy,axis=1)       # greedy action per state\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.bar(np.arange(nS),actions)\n",
        "    title=\"Greedy Policy\"\n",
        "    if gamma_label:\n",
        "        title+=\" (gamma=\"+gamma_label+\")\"\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"State\")\n",
        "    plt.ylabel(\"Action (0=Left,1=Down,2=Right,3=Up)\")\n",
        "    plt.show()\n",
        "\n",
        "### Running VI\n",
        "if __name__==\"__main__\":\n",
        "    gammas=[0.9,0.99,0.6]   # test different discount factors\n",
        "    all_V={}\n",
        "    all_iters=[]\n",
        "\n",
        "    for gamma in gammas:\n",
        "        print(\"\\n=== Running Value Iteration gamma=\",gamma,\"===\")\n",
        "        V_opt,policy_opt,iterations=value_iteration(env,discount_factor=gamma)\n",
        "        all_V[gamma]=V_opt\n",
        "        all_iters.append(iterations)\n",
        "\n",
        "        print(\"Converged in\",iterations,\"iterations\")\n",
        "\n",
        "        plot_values(env,V_opt,gamma_label=str(gamma))\n",
        "        plot_policy(env,policy_opt,gamma_label=str(gamma))\n",
        "\n",
        "    # compare V* curves for different gammas\n",
        "    plt.figure(figsize=(7,3))\n",
        "    for gamma in gammas:\n",
        "        plt.plot(all_V[gamma],label=\"gamma=\"+str(gamma))\n",
        "    plt.title(\"Value Functions Comparison\")\n",
        "    plt.xlabel(\"State\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # compare how many iterations each gamma took\n",
        "    plt.figure(figsize=(5,3))\n",
        "    plt.bar([str(g) for g in gammas],all_iters)\n",
        "    plt.title(\"Iterations vs Gamma\")\n",
        "    plt.xlabel(\"Gamma\")\n",
        "    plt.ylabel(\"Iterations\")\n",
        "    plt.show()\n",
        "\n",
        "    ## for gamma larger value function larger. care moe about future rewards\n",
        "    ## larger gamma ore iterrations"
      ]
    }
  ]
}