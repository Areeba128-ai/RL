{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE5EE-H-rjky"
      },
      "outputs": [],
      "source": [
        "def EnvironmentSetup():\n",
        "    import gymnasium as gym\n",
        "    import numpy as np\n",
        "    if not hasattr(np,\"bool8\"):np.bool8=np.bool_\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    env=gym.make('FrozenLake-v1',is_slippery=False)  # deterministic environment\n",
        "    state=env.reset()\n",
        "\n",
        "    print(\"Initial State:\",state)\n",
        "    print(\"Action Space:\",env.action_space)\n",
        "    print(\"Observation Space:\",env.observation_space)\n",
        "    print(\"Actions: 0=Left, 1=Down, 2=Right, 3=Up\")\n",
        "\n",
        "    return env,state\n",
        "env,state=EnvironmentSetup()\n",
        "\n",
        "def InteractionLoop(env,num_episodes=5):\n",
        "    for episode in range(num_episodes):\n",
        "        state=env.reset()\n",
        "        terminated=False\n",
        "        total_reward=0\n",
        "        step_count=0\n",
        "        while not terminated:\n",
        "            action=env.action_space.sample()\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            terminated = terminated or truncated   # merge flags\n",
        "            total_reward+=reward\n",
        "            print(f\"Step {step_count}: State={state}, Action={action}, Reward={reward}, Next State={next_state}, Terminated={terminated}\")\n",
        "            state=next_state\n",
        "            step_count+=1\n",
        "        print(f\"Episode {episode+1} ended with total reward: {total_reward}\\n\")\n",
        "InteractionLoop(env,5)\n",
        "\n",
        "############\n",
        "def VisualizePathComparison(env,size=4,num_episodes=3):\n",
        "    import numpy as np\n",
        "\n",
        "    def visualize_path(path,size=4):\n",
        "        grid=np.full((size,size),'-')\n",
        "        visit_count=np.zeros((size,size),dtype=int)\n",
        "        for step,state in enumerate(path):\n",
        "            row,col=divmod(state,size)\n",
        "            grid[row,col]=str(step)\n",
        "            visit_count[row,col]+=1\n",
        "        print(\"Path Grid (Step Order):\")\n",
        "        print(grid)\n",
        "        print(\"\\nVisit Count Grid:\")\n",
        "        print(visit_count)\n",
        "        most_visited=np.unravel_index(np.argmax(visit_count),(size,size))\n",
        "        print(f\"\\nMost visited cell: {most_visited} visited {visit_count[most_visited]} times\")\n",
        "\n",
        "    paths=[]\n",
        "    steps_list=[]\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        state=env.reset()\n",
        "        if isinstance(state,tuple):state=state[0]\n",
        "        terminated=False\n",
        "        path=[state]\n",
        "        step_count=0\n",
        "        while not terminated:\n",
        "            action=env.action_space.sample()\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "            terminated = terminated or truncated\n",
        "            path.append(state)\n",
        "            step_count+=1\n",
        "        print(f\"\\nEpisode {ep+1} finished in {step_count} steps\")\n",
        "        visualize_path(path)\n",
        "        paths.append(path)\n",
        "        steps_list.append(step_count)\n",
        "\n",
        "    print(\"\\n--- Path Efficiency Comparison ---\")\n",
        "    for i,steps in enumerate(steps_list):\n",
        "        print(f\"Episode {i+1}: {steps} steps\")\n",
        "    best_ep=np.argmin(steps_list)\n",
        "    print(f\"\\nMost efficient path: Episode {best_ep+1} with {steps_list[best_ep]} steps\")\n",
        "\n",
        "VisualizePathComparison(env,size=4,num_episodes=3)\n",
        "#####\n",
        "\n",
        "def VisualizePathRun(env,size=4):\n",
        "    import numpy as np\n",
        "    def visualize_path(path,size=4):\n",
        "        grid=np.full((size,size),'-')\n",
        "        for step,state in enumerate(path):\n",
        "            row,col=divmod(state,size)\n",
        "            grid[row,col]=str(step)\n",
        "        print(grid)\n",
        "\n",
        "    state=env.reset()\n",
        "    if isinstance(state,tuple):state=state[0]\n",
        "    terminated=False\n",
        "    path=[state]\n",
        "    while not terminated:\n",
        "        action=env.action_space.sample()\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        terminated = terminated or truncated   # merge flags\n",
        "        path.append(state)\n",
        "    visualize_path(path)\n",
        "path_return=VisualizePathRun(env)\n",
        "\n",
        "def TrackCumulativeRewards(env,n_episodes=10,plot=True):\n",
        "    import matplotlib.pyplot as plt\n",
        "    rewards=[]\n",
        "    for episode in range(n_episodes):\n",
        "        state=env.reset()\n",
        "        if isinstance(state,tuple):state=state[0]\n",
        "        terminated=False\n",
        "        total_reward=0\n",
        "        while not terminated:\n",
        "            action=env.action_space.sample()\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            terminated = terminated or truncated   # merge flags\n",
        "            total_reward+=reward\n",
        "        rewards.append(total_reward)\n",
        "    if plot:\n",
        "        plt.figure()\n",
        "        plt.plot(rewards)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Total Reward\")\n",
        "        plt.title(\"Random Policy Reward per Episode\")\n",
        "        plt.show()\n",
        "    return rewards\n",
        "rewards=TrackCumulativeRewards(env,n_episodes=10,plot=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0q7V1GvtsD90"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}