{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lImXx7bcsm3N"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "# Step 1 – Environment setup\n",
        "env = gym.make('Taxi-v3', render_mode='ansi')\n",
        "obs, info = env.reset()\n",
        "\n",
        "print(\"Initial State:\", obs)\n",
        "print(\"Observation space size:\", env.observation_space.n)  # 500 states\n",
        "print(\"Action space size:\", env.action_space.n)            # 6 actions\n",
        "\n",
        "# Original transition model (keep a copy)\n",
        "P_orig = env.unwrapped.P\n",
        "reward_min = min({r for s in P_orig for a in P_orig[s] for (_, _, r, _) in P_orig[s][a]})\n",
        "reward_max = max({r for s in P_orig for a in P_orig[s] for (_, _, r, _) in P_orig[s][a]})\n",
        "print(\"Reward range:\", (reward_min, reward_max))\n",
        "\n",
        "print(env.render())\n",
        "\n",
        "# -------- helper: make modified reward model --------\n",
        "def modify_rewards(P_base, time_penalty=-1, illegal_penalty=-10, success_reward=20):\n",
        "    \"\"\"\n",
        "    Returns a deep-copied transition model with modified rewards.\n",
        "\n",
        "    Taxi-v3 default:\n",
        "      - step:      -1\n",
        "      - illegal:  -10\n",
        "      - success:  +20\n",
        "    We map these to new values.\n",
        "    \"\"\"\n",
        "    P_new = copy.deepcopy(P_base)\n",
        "    for s in P_new:\n",
        "        for a in P_new[s]:\n",
        "            new_list = []\n",
        "            for prob, next_state, reward, done in P_new[s][a]:\n",
        "                if reward == -1:\n",
        "                    r_new = time_penalty\n",
        "                elif reward == -10:\n",
        "                    r_new = illegal_penalty\n",
        "                elif reward == 20:\n",
        "                    r_new = success_reward\n",
        "                else:\n",
        "                    r_new = reward\n",
        "                new_list.append((prob, next_state, r_new, done))\n",
        "            P_new[s][a] = new_list\n",
        "    return P_new\n",
        "\n",
        "# -------- Step 3 – Value Iteration (now takes P as argument) --------\n",
        "def value_iteration(env, P, discount_factor=0.99, theta=1e-6, max_iterations=10000):\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "    V = np.zeros(nS)\n",
        "\n",
        "    deltas = []  # record max change each iteration\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        delta = 0.0\n",
        "        for s in range(nS):\n",
        "            q_sa = np.zeros(nA)\n",
        "            for a in range(nA):\n",
        "                for prob, next_state, reward, done in P[s][a]:\n",
        "                    q_sa[a] += prob * (reward + discount_factor * V[next_state])\n",
        "            new_v = np.max(q_sa)\n",
        "            delta = max(delta, abs(new_v - V[s]))\n",
        "            V[s] = new_v\n",
        "        deltas.append(delta)\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    policy = extract_policy_from_v(env, P, V, discount_factor)\n",
        "    return V, policy, i + 1, deltas\n",
        "\n",
        "# -------- Step 4 – Extract policy from V (also uses P) --------\n",
        "def extract_policy_from_v(env, P, V, discount_factor=0.99):\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "    policy = np.zeros((nS, nA))\n",
        "\n",
        "    for s in range(nS):\n",
        "        q_sa = np.zeros(nA)\n",
        "        for a in range(nA):\n",
        "            for prob, next_state, reward, done in P[s][a]:\n",
        "                q_sa[a] += prob * (reward + discount_factor * V[next_state])\n",
        "        best_action = np.argmax(q_sa)\n",
        "        policy[s] = np.eye(nA)[best_action]\n",
        "    return policy\n",
        "\n",
        "# -------- Evaluate policy: success rate + avg steps (using real env) --------\n",
        "def evaluate_policy(env, policy, n_episodes=500):\n",
        "    success = 0\n",
        "    total_steps_success = 0\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = np.argmax(policy[obs])\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            steps += 1\n",
        "\n",
        "            # success = reached passenger drop-off successfully (reward > 0 at end)\n",
        "            if done and reward > 0:\n",
        "                success += 1\n",
        "                total_steps_success += steps\n",
        "\n",
        "    success_rate = success / n_episodes\n",
        "    avg_steps_success = (total_steps_success / success) if success > 0 else float('inf')\n",
        "    return success_rate, avg_steps_success\n",
        "\n",
        "# -------- Step 5 – Plot helpers --------\n",
        "def plot_values(env, V, label=\"\"):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(V)\n",
        "    title = \"Value Function for Taxi-v3\"\n",
        "    if label:\n",
        "        title += f\" ({label})\"\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"State (0–499)\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_policy(env, policy, label=\"\"):\n",
        "    nS = env.observation_space.n\n",
        "    actions = np.argmax(policy, axis=1)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.bar(np.arange(nS), actions)\n",
        "    title = \"Greedy Policy (Best Action per State)\"\n",
        "    if label:\n",
        "        title += f\" ({label})\"\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"State Index\")\n",
        "    plt.ylabel(\"Action (0–5)\")\n",
        "    plt.show()\n",
        "\n",
        "# -------- Step 6 – Test different reward structures --------\n",
        "if __name__ == \"__main__\":\n",
        "    gamma = 0.99  # keep gamma fixed; change only reward structure\n",
        "\n",
        "    # (name, time_penalty, illegal_penalty, success_reward)\n",
        "    reward_settings = [\n",
        "        (\"default\",        -1,  -10,  20),\n",
        "        (\"low_time_cost\",  -0.2, -10, 20),\n",
        "        (\"harsh_illegal\",  -1,  -30, 20),\n",
        "        (\"high_success\",   -1,  -10, 40),\n",
        "    ]\n",
        "\n",
        "    all_V = {}\n",
        "    all_iters = {}\n",
        "    success_rates = {}\n",
        "    avg_steps = {}\n",
        "    all_deltas = {}\n",
        "\n",
        "    for name, t_pen, ill_pen, succ_rew in reward_settings:\n",
        "        print(f\"\\n=== Running Value Iteration with reward setting: {name} ===\")\n",
        "        P_mod = modify_rewards(P_orig,\n",
        "                               time_penalty=t_pen,\n",
        "                               illegal_penalty=ill_pen,\n",
        "                               success_reward=succ_rew)\n",
        "\n",
        "        V_opt, policy_opt, iters, deltas = value_iteration(\n",
        "            env, P_mod, discount_factor=gamma\n",
        "        )\n",
        "\n",
        "        all_V[name] = V_opt\n",
        "        all_iters[name] = iters\n",
        "        all_deltas[name] = deltas\n",
        "\n",
        "        print(f\"Converged in {iters} iterations for setting '{name}'\")\n",
        "\n",
        "        rate, avg_steps_succ = evaluate_policy(env, policy_opt, n_episodes=500)\n",
        "        success_rates[name] = rate\n",
        "        avg_steps[name] = avg_steps_succ\n",
        "\n",
        "        print(f\"Success rate ({name}): {rate*100:.2f}%\")\n",
        "        print(f\"Avg steps / successful episode ({name}): {avg_steps_succ:.2f}\")\n",
        "\n",
        "        plot_values(env, V_opt, label=name)\n",
        "        plot_policy(env, policy_opt, label=name)\n",
        "\n",
        "    # --- Comparison plots ---\n",
        "\n",
        "    # Value iteration convergence (delta)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for name in reward_settings:\n",
        "        label = name[0]  # first element of tuple is the name\n",
        "        plt.plot(all_deltas[label], label=label)\n",
        "    plt.title(\"Value Iteration Convergence for Different Reward Structures\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Delta (max |V_new - V_old|)\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Success rate vs reward structure\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    labels = [cfg[0] for cfg in reward_settings]\n",
        "    sr_vals = [success_rates[name] * 100 for name in labels]\n",
        "    plt.bar(labels, sr_vals)\n",
        "    plt.title(\"Success Rate vs Reward Structure\")\n",
        "    plt.ylabel(\"Success Rate (%)\")\n",
        "    plt.xticks(rotation=20)\n",
        "    plt.show()\n",
        "\n",
        "    # Avg steps vs reward structure\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    steps_vals = [avg_steps[name] for name in labels]\n",
        "    plt.bar(labels, steps_vals)\n",
        "    plt.title(\"Average Steps per Successful Episode vs Reward Structure\")\n",
        "    plt.ylabel(\"Avg Steps (successful episodes)\")\n",
        "    plt.xticks(rotation=20)\n",
        "    plt.show()\n",
        "\n",
        "    # Iterations to converge vs reward structure\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    it_vals = [all_iters[name] for name in labels]\n",
        "    plt.bar(labels, it_vals)\n",
        "    plt.title(\"Convergence Speed vs Reward Structure\")\n",
        "    plt.ylabel(\"Iterations to Converge\")\n",
        "    plt.xticks(rotation=20)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VyQAaRHcLZAi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}