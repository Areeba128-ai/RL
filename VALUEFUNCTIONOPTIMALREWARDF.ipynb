{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lImXx7bcsm3N"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "np.set_printoptions(precision=3,suppress=True)\n",
        "\n",
        "# ========= 1) Environment setup =========\n",
        "env=gym.make('FrozenLake-v1',is_slippery=True,render_mode='ansi')\n",
        "obs,info=env.reset(seed=0)\n",
        "\n",
        "print(\"Initial State:\",obs)\n",
        "print(\"Observation space size:\",env.observation_space.n)  # 16 states (4x4)\n",
        "print(\"Action space size:\",env.action_space.n)            # 4 actions (L,D,R,U)\n",
        "\n",
        "P_orig=env.unwrapped.P\n",
        "reward_min=min({r for s in P_orig for a in P_orig[s] for (_,_,r,_) in P_orig[s][a]})\n",
        "reward_max=max({r for s in P_orig for a in P_orig[s] for (_,_,r,_) in P_orig[s][a]})\n",
        "print(\"Reward range (env default):\",(reward_min,reward_max))\n",
        "\n",
        "print(env.render())  # text map\n",
        "\n",
        "desc=env.unwrapped.desc              # 2D array of bytes: b'S', b'F', b'H', b'G'\n",
        "flat_desc=desc.flatten()             # map next_state -> tile char\n",
        "\n",
        "\n",
        "# ========= 2) Helper: modify rewards in transition model =========\n",
        "def modify_rewards_frozenlake(P_base,flat_desc,\n",
        "                              step_reward=0.0,\n",
        "                              hole_reward=0.0,\n",
        "                              goal_reward=1.0):\n",
        "    \"\"\"\n",
        "    Returns a deep-copied transition model with modified rewards\n",
        "    depending on what tile the NEXT STATE is:\n",
        "\n",
        "      S/F -> step_reward\n",
        "      H   -> hole_reward\n",
        "      G   -> goal_reward\n",
        "    \"\"\"\n",
        "    P_new=copy.deepcopy(P_base)\n",
        "    for s in P_new:\n",
        "        for a in P_new[s]:\n",
        "            new_list=[]\n",
        "            for prob,next_state,old_r,done in P_new[s][a]:\n",
        "                tile=flat_desc[next_state].decode('utf-8')\n",
        "                if tile=='G':\n",
        "                    r_new=goal_reward\n",
        "                elif tile=='H':\n",
        "                    r_new=hole_reward\n",
        "                else:\n",
        "                    r_new=step_reward\n",
        "                new_list.append((prob,next_state,r_new,done))\n",
        "            P_new[s][a]=new_list\n",
        "    return P_new\n",
        "\n",
        "\n",
        "# ========= 3) Value Iteration (takes P) =========\n",
        "def value_iteration(env,P,discount_factor=0.99,theta=1e-8,max_iterations=1000):\n",
        "    nS=env.observation_space.n\n",
        "    nA=env.action_space.n\n",
        "    V=np.zeros(nS)\n",
        "    deltas=[]\n",
        "\n",
        "    for it in range(max_iterations):\n",
        "        delta=0.0\n",
        "        for s in range(nS):\n",
        "            q_sa=np.zeros(nA)\n",
        "            for a in range(nA):\n",
        "                for prob,next_state,reward,done in P[s][a]:\n",
        "                    q_sa[a]+=prob*(reward+discount_factor*V[next_state])\n",
        "            new_v=np.max(q_sa)\n",
        "            delta=max(delta,abs(new_v-V[s]))\n",
        "            V[s]=new_v\n",
        "        deltas.append(delta)\n",
        "        if delta<theta:\n",
        "            break\n",
        "\n",
        "    policy=extract_policy_from_v(env,P,V,discount_factor)\n",
        "    return V,policy,it+1,deltas\n",
        "\n",
        "\n",
        "# ========= 4) Policy extraction (uses P and V) =========\n",
        "def extract_policy_from_v(env,P,V,discount_factor=0.99):\n",
        "    nS=env.observation_space.n\n",
        "    nA=env.action_space.n\n",
        "    policy=np.zeros((nS,nA))\n",
        "    for s in range(nS):\n",
        "        q_sa=np.zeros(nA)\n",
        "        for a in range(nA):\n",
        "            for prob,next_state,reward,done in P[s][a]:\n",
        "                q_sa[a]+=prob*(reward+discount_factor*V[next_state])\n",
        "        best_action=np.argmax(q_sa)\n",
        "        policy[s]=np.eye(nA)[best_action]\n",
        "    return policy\n",
        "\n",
        "\n",
        "# ========= 5) Evaluate policy on real env =========\n",
        "def evaluate_policy(env,policy,n_episodes=500):\n",
        "    success=0\n",
        "    total_steps_success=0\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        obs,_=env.reset()\n",
        "        done=False\n",
        "        steps=0\n",
        "\n",
        "        while not done:\n",
        "            action=np.argmax(policy[obs])\n",
        "            obs,reward,terminated,truncated,_=env.step(action)\n",
        "            done=terminated or truncated\n",
        "            steps+=1\n",
        "            if done and reward>0:  # reached goal G\n",
        "                success+=1\n",
        "                total_steps_success+=steps\n",
        "\n",
        "    success_rate=success/n_episodes\n",
        "    avg_steps_success=(total_steps_success/success) if success>0 else float('inf')\n",
        "    return success_rate,avg_steps_success\n",
        "\n",
        "\n",
        "# ========= 6) Plot helpers =========\n",
        "def plot_grid_values_and_policy(env,V,policy,label=\"\"):\n",
        "    desc=env.unwrapped.desc\n",
        "    nrow,ncol=desc.shape\n",
        "    V_grid=V.reshape((nrow,ncol))\n",
        "\n",
        "    plt.figure(figsize=(3,3))\n",
        "    plt.imshow(V_grid,cmap='cool',alpha=0.7)\n",
        "    ax=plt.gca()\n",
        "\n",
        "    arrow_dict={0:'←',1:'↓',2:'→',3:'↑'}\n",
        "\n",
        "    for x in range(ncol+1):\n",
        "        ax.axvline(x-0.5,lw=0.5,color='black')\n",
        "    for y in range(nrow+1):\n",
        "        ax.axhline(y-0.5,lw=0.5,color='black')\n",
        "\n",
        "    for r in range(nrow):\n",
        "        for c in range(ncol):\n",
        "            s=r*ncol+c\n",
        "            tile=desc[r,c].decode('utf-8')\n",
        "            v=V[s]\n",
        "\n",
        "            if tile=='H':color='red'\n",
        "            elif tile=='G':color='green'\n",
        "            elif tile=='S':color='blue'\n",
        "            else:color='black'\n",
        "\n",
        "            plt.text(c,r,tile,ha='center',va='center',\n",
        "                     color=color,fontsize=10,fontweight='bold')\n",
        "            if tile!='H':\n",
        "                plt.text(c,r+0.3,f\"{v:.2f}\",ha='center',va='center',\n",
        "                         color='black',fontsize=6)\n",
        "\n",
        "            best_action=np.argmax(policy[s])\n",
        "            plt.text(c,r-0.25,arrow_dict[best_action],\n",
        "                     ha='center',va='center',color='purple',fontsize=12)\n",
        "\n",
        "    title=\"FrozenLake: V and π*\"\n",
        "    if label:\n",
        "        title+=f\" ({label})\"\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ========= 7) Test different reward structures =========\n",
        "if __name__==\"__main__\":\n",
        "    gamma=0.99\n",
        "\n",
        "    # (name, step_reward, hole_reward, goal_reward)\n",
        "    reward_settings=[\n",
        "        (\"default\",        0.0,   0.0,   1.0),   # original env\n",
        "        (\"step_penalty\",  -0.05,  0.0,   1.0),   # penalize every move\n",
        "        (\"hole_penalty\",   0.0,  -1.0,   1.0),   # strongly punish falling into holes\n",
        "        (\"high_goal\",      0.0,   0.0,   5.0),   # larger reward for reaching goal\n",
        "    ]\n",
        "\n",
        "    all_V={}\n",
        "    all_iters={}\n",
        "    all_deltas={}\n",
        "    success_rates={}\n",
        "    avg_steps={}\n",
        "\n",
        "    for name,step_r,hole_r,goal_r in reward_settings:\n",
        "        print(f\"\\n=== Running Value Iteration with reward setting: {name} ===\")\n",
        "        P_mod=modify_rewards_frozenlake(P_orig,flat_desc,\n",
        "                                        step_reward=step_r,\n",
        "                                        hole_reward=hole_r,\n",
        "                                        goal_reward=goal_r)\n",
        "\n",
        "        V_opt,policy_opt,iters,deltas=value_iteration(\n",
        "            env,P_mod,discount_factor=gamma\n",
        "        )\n",
        "\n",
        "        all_V[name]=V_opt\n",
        "        all_iters[name]=iters\n",
        "        all_deltas[name]=deltas\n",
        "\n",
        "        print(f\"Converged in {iters} iterations for setting '{name}'\")\n",
        "\n",
        "        rate,avg_steps_succ=evaluate_policy(env,policy_opt,n_episodes=500)\n",
        "        success_rates[name]=rate\n",
        "        avg_steps[name]=avg_steps_succ\n",
        "\n",
        "        print(f\"Success rate ({name}): {rate*100:.2f}%\")\n",
        "        print(f\"Avg steps / successful episode ({name}): {avg_steps_succ:.2f}\")\n",
        "\n",
        "        plot_grid_values_and_policy(env,V_opt,policy_opt,label=name)\n",
        "\n",
        "    # ---- Convergence (delta) comparison ----\n",
        "    plt.figure(figsize=(8,4))\n",
        "    for cfg in reward_settings:\n",
        "        name=cfg[0]\n",
        "        plt.plot(all_deltas[name],label=name)\n",
        "    plt.title(\"Value Iteration Convergence for Different Reward Structures (FrozenLake)\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Delta (max |V_new - V_old|)\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # ---- Success rate vs reward structure ----\n",
        "    labels=[cfg[0] for cfg in reward_settings]\n",
        "    sr_vals=[success_rates[name]*100 for name in labels]\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.bar(labels,sr_vals)\n",
        "    plt.title(\"Success Rate vs Reward Structure (FrozenLake)\")\n",
        "    plt.ylabel(\"Success Rate (%)\")\n",
        "    plt.xticks(rotation=20)\n",
        "    plt.show()\n",
        "\n",
        "    # ---- Average steps vs reward structure ----\n",
        "    steps_vals=[avg_steps[name] for name in labels]\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.bar(labels,steps_vals)\n",
        "    plt.title(\"Average Steps per Successful Episode vs Reward Structure\")\n",
        "    plt.ylabel(\"Avg Steps (successful episodes)\")\n",
        "    plt.xticks(rotation=20)\n",
        "    plt.show()\n",
        "\n",
        "    # ---- Iterations to converge vs reward structure ----\n",
        "    it_vals=[all_iters[name] for name in labels]\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.bar(labels,it_vals)\n",
        "    plt.title(\"Convergence Speed vs Reward Structure (FrozenLake)\")\n",
        "    plt.ylabel(\"Iterations to Converge\")\n",
        "    plt.xticks(rotation=20)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VyQAaRHcLZAi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}