{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lImXx7bcsm3N"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(precision=3,suppress=True)  # show 3 decimal, no scientific format\n",
        "\n",
        "# Create FrozenLake environment (slippery)\n",
        "env=gym.make('FrozenLake-v1',is_slippery=True,render_mode='ansi')  # 4x4 lake, stochastic\n",
        "\n",
        "obs,info=env.reset()\n",
        "print(\"Initial State:\",obs)\n",
        "print(\"Action Space:\",env.action_space)           # Discrete(4) -> 0=L,1=D,2=R,3=U\n",
        "print(\"Observation Space:\",env.observation_space) # Discrete(16) -> 16 states in 4x4 grid\n",
        "print(\"Grid shape (rows, cols):\",(4,4))\n",
        "\n",
        "# Reward range from transition model\n",
        "P=env.unwrapped.P   # full transition dictionary P[s][a] -> list of (prob,next_state,reward,done)\n",
        "reward_min=min({r for s in P for a in P[s] for (_,_,r,_) in P[s][a]})\n",
        "reward_max=max({r for s in P for a in P[s] for (_,_,r,_) in P[s][a]})\n",
        "print(\"Reward range:\",(reward_min,reward_max))\n",
        "\n",
        "# Render text version of the lake\n",
        "frame=env.render()  # shows S, F, H, G as text\n",
        "print(frame)\n",
        "\n",
        "gamma=0.99\n",
        "\n",
        "# helper: compute Q from V\n",
        "def q_from_v(env,V,s,gamma=1.0):\n",
        "    P=env.unwrapped.P\n",
        "    nA=env.action_space.n\n",
        "    q=np.zeros(nA)                 # q[a] will store Q(s,a)\n",
        "    for a in range(nA):            # loop over all actions\n",
        "        for prob,next_state,reward,done in P[s][a]:  # sum over all possible next states\n",
        "            q[a]+=prob*(reward+gamma*V[next_state])  # Bellman expectation formula\n",
        "    return q\n",
        "\n",
        "# simple one-step policy improvement\n",
        "def simple_policy_improvement(env,policy,V,gamma=0.99):\n",
        "    P=env.unwrapped.P\n",
        "    nS=env.observation_space.n\n",
        "    nA=env.action_space.n\n",
        "    policy_new=np.zeros_like(policy)\n",
        "\n",
        "    for s in range(nS):\n",
        "        old_action=np.argmax(policy[s])  # action with highest prob in old policy\n",
        "\n",
        "        # compute Q(s,a) for all actions\n",
        "        Q_sa=np.zeros(nA)\n",
        "        for a in range(nA):\n",
        "            for prob,next_state,reward,done in P[s][a]:\n",
        "                Q_sa[a]+=prob*(reward+gamma*V[next_state])\n",
        "\n",
        "        best_action=np.argmax(Q_sa)      # greedy argmax_a Σ_s' P(...) [R + γV]\n",
        "\n",
        "        policy_new[s]=np.eye(nA)[best_action]  # deterministic one-hot\n",
        "\n",
        "        print(f\"State {s:2d}: old_action={old_action}, new_action={best_action}\")\n",
        "\n",
        "    return policy_new\n",
        "\n",
        "# plotting helper: show arrows for a policy\n",
        "def plot_policy(V,policy,title=\"Policy\",draw_vals=False):\n",
        "    nrow=env.unwrapped.nrow  # 4\n",
        "    ncol=env.unwrapped.ncol  # 4\n",
        "    arrow_symbols={0:'←',1:'↓',2:'→',3:'↑'}  # mapping action index to arrow\n",
        "\n",
        "    grid=V.reshape((nrow,ncol))\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(grid,cmap='cool',interpolation='none')\n",
        "\n",
        "    for s in range(nrow*ncol):\n",
        "        row,col=divmod(s,ncol)\n",
        "        best_action=np.argmax(policy[s])\n",
        "\n",
        "        if draw_vals:\n",
        "            plt.text(col,row,f'{V[s]:.2f}',ha='center',va='center',\n",
        "                     color='white',fontsize=10)\n",
        "        plt.text(col,row,arrow_symbols[best_action],ha='center',va='center',\n",
        "                 color='yellow',fontsize=14)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# run: one-step improvement and plots\n",
        "\n",
        "nS=env.observation_space.n\n",
        "nA=env.action_space.n\n",
        "\n",
        "# example value function (here random, in real case from evaluation / value-iteration)\n",
        "V=np.random.rand(nS)\n",
        "print(\"\\nExample V(s) (4x4):\")\n",
        "print(V.reshape(4,4))\n",
        "\n",
        "# old policy: uniform random over actions\n",
        "policy_old=np.ones((nS,nA))/nA\n",
        "print(\"\\nOld policy (row=state, cols=actions 0..3):\")\n",
        "print(policy_old)\n",
        "\n",
        "print(\"\\n--- Running one-step policy improvement ---\")\n",
        "policy_new=simple_policy_improvement(env,policy_old,V,gamma)\n",
        "\n",
        "print(\"\\nNew policy (row=state, cols=actions 0..3):\")\n",
        "print(policy_new)\n",
        "\n",
        "# plot old vs new policy\n",
        "plot_policy(V,policy_old,title=\"Old Policy (before improvement)\",draw_vals=True)\n",
        "plot_policy(V,policy_new,title=\"New Policy (after improvement)\",draw_vals=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B0h6b5Zxw8MZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}