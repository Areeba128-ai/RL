{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr5slretwF8P"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create environment\n",
        "env=gym.make(\"FrozenLake-v1\",is_slippery=True)\n",
        "\n",
        "# Monte Carlo Prediction\n",
        "def mc_prediction(env,policy,episodes=10000,gamma=0.99):\n",
        "    V=np.zeros(env.observation_space.n)\n",
        "    returns={s:[] for s in range(env.observation_space.n)}  #dict where each state s has list of all observed return from s\n",
        "    V_track=[] # tracking, for ploingt\n",
        "    ep_rewards=[]  # store total reward per episode\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        episode=[] #stor list stte,reward pairs\n",
        "        state,_=env.reset()\n",
        "        done=False\n",
        "        total_reward=0.0\n",
        "\n",
        "        # Generate an episode following the policy\n",
        "        while not done:\n",
        "            action=policy[state] #pick actin accoring to policy\n",
        "            next_state,reward,terminated,truncated,_=env.step(action)\n",
        "            done=terminated or truncated\n",
        "            episode.append((state,reward))\n",
        "            total_reward+=reward\n",
        "            state=next_state #after after while loop you have one fill episode [(s0,r1), (s1,r2),..]\n",
        "\n",
        "        # Compute returns (first-visit MC)\n",
        "        G=0.0 #cumulative discounted return\n",
        "        visited_states=set() # so that there ar no duplocaetes\n",
        "        for s,r in reversed(episode): #iterting backward thrugh episode\n",
        "            G=gamma*G+r #gt=rt+gamma*g(t+1)\n",
        "            if s not in visited_states: #only updae if first time never visted\n",
        "                returns[s].append(G)\n",
        "                V[s]=np.mean(returns[s]) # as MC is avergae\n",
        "                visited_states.add(s)\n",
        "\n",
        "        V_track.append(V.copy())\n",
        "        ep_rewards.append(total_reward)\n",
        "\n",
        "    return V,V_track,ep_rewards #v is final value function, v_track 10000 track\n",
        "\n",
        "# Temporal Difference (TD(0))\n",
        "def td_prediction(env,policy,episodes=10000,alpha=0.1,gamma=0.99): #alpha=learning rate\n",
        "    V=np.zeros(env.observation_space.n)\n",
        "    V_track=[]\n",
        "    ep_rewards=[]  # store total reward per episode\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state,_=env.reset()\n",
        "        done=False\n",
        "        total_reward=0.0\n",
        "\n",
        "        while not done:\n",
        "            action=policy[state]\n",
        "            next_state,reward,terminated,truncated,_=env.step(action)\n",
        "            done=terminated or truncated\n",
        "            total_reward+=reward\n",
        "\n",
        "            # TD(0) update\n",
        "            V[state]=V[state]+alpha*(reward+gamma*V[next_state]-V[state]) # V(st​) <- V(st​)+alpha[r(t+1)​+γV(st+1​)−V(st​)]\n",
        "\n",
        "            state=next_state\n",
        "\n",
        "        V_track.append(V.copy())\n",
        "        ep_rewards.append(total_reward)\n",
        "\n",
        "    return V,V_track,ep_rewards\n",
        "\n",
        "# Define Random Policy\n",
        "#policy={s:np.random.choice([0,1,2,3]) for s in range(env.observation_space.n)} # make dic having state: any random acton 0:left, 1:down so onn..\n",
        "\n",
        "policy = {s: 2 for s in range(16)}  #policy always moves right\n",
        "\n",
        "# Compute Value Functions once with default hyperparameters\n",
        "V_mc,V_mc_track,rewards_mc=mc_prediction(env,policy)\n",
        "V_td,V_td_track,rewards_td=td_prediction(env,policy)\n",
        "\n",
        "\n",
        "# Plotting Convergence\n",
        "def plot_convergence(V_track,title):\n",
        "    plt.figure(figsize=(8,5)) #take value of state s from every snapshot stored across episodes\n",
        "    for s in range(env.observation_space.n):\n",
        "        values=[v[s] for v in V_track]\n",
        "        plt.plot(values,label=f\"State {s}\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Value Estimate V(s)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show() #plot how v(s) changes over time\n",
        "\n",
        "# NEW: plot episode rewards + running average\n",
        "def plot_rewards(ep_rewards,title):\n",
        "    plt.figure(figsize=(8,5))\n",
        "    episodes=len(ep_rewards)\n",
        "    avg_rewards=np.cumsum(ep_rewards)/np.arange(1,episodes+1)  # running average\n",
        "    plt.plot(ep_rewards,alpha=0.3,label=\"Episode reward\")\n",
        "    plt.plot(avg_rewards,label=\"Average reward\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ifferent alpha and gamma values\n",
        "\n",
        "gammas=[0.9,0.99]\n",
        "alphas=[0.05,0.2]\n",
        "\n",
        "#  Monte Carlo: effect of gamma (no alpha in MC)\n",
        "for g in gammas:\n",
        "    V_mc_g,V_mc_track_g,rewards_mc_g=mc_prediction(env,policy,episodes=10000,gamma=g)\n",
        "    plot_convergence(V_mc_track_g,f\"Monte Carlo Value Estimates (gamma={g})\")\n",
        "    plot_rewards(rewards_mc_g,f\"Monte Carlo Episode Rewards (gamma={g})\")\n",
        "\n",
        "# TD(0): effect of both alpha and gamma\n",
        "for g in gammas:\n",
        "    for a in alphas:\n",
        "        V_td_ga,V_td_track_ga,rewards_td_ga=td_prediction(env,policy,episodes=10000,\n",
        "                                            alpha=a,gamma=g)\n",
        "        title=f\"TD(0) Value Estimates (alpha={a}, gamma={g})\"\n",
        "        plot_convergence(V_td_track_ga,title)\n",
        "        plot_rewards(rewards_td_ga,f\"TD(0) Episode Rewards (alpha={a}, gamma={g})\")\n",
        "\n",
        "# when everything random, plocy deterrminic never reach goal reward =0.\n",
        "# some states may reaxh goal. value chnages grahs vigly as agent going here and there.\n",
        "# in md when gamma raete values becomes larger as shown in graph. stable but slow.\n",
        "#in td:\n",
        "#low learning rate = slow but stable learning\n",
        "# high alpah= fas updayes but unstable\n",
        "\n",
        "# in rewatds\"\n",
        "#it become smooth over time total reward whch is averge\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "weka7-AY8zDZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}