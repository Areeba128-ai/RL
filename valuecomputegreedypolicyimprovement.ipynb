{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lImXx7bcsm3N"
      },
      "outputs": [],
      "source": [
        "### Code cell 0 ###\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(precision=3,suppress=True)  # pretty printing of numpy arrays\n",
        "\n",
        "# 1) Environment setup\n",
        "env=gym.make('FrozenLake-v1',is_slippery=True,render_mode='ansi')  # 4x4 slippery lake\n",
        "obs,info=env.reset()\n",
        "\n",
        "print(\"Initial State:\",obs)\n",
        "print(\"Action Space:\",env.action_space)           # Discrete(4)\n",
        "print(\"Observation Space:\",env.observation_space) # Discrete(16)\n",
        "print(\"Grid shape (rows, cols):\",(4,4))\n",
        "\n",
        "# reward range from transition model\n",
        "P=env.unwrapped.P  # transition model: P[s][a] = list of (prob,next_state,reward,done)\n",
        "reward_min=min({r for s in P for a in P[s] for (_,_,r,_) in P[s][a]})\n",
        "reward_max=max({r for s in P for a in P[s] for (_,_,r,_) in P[s][a]})\n",
        "print(\"Reward range:\",(reward_min,reward_max))\n",
        "\n",
        "# show text rendering of the map\n",
        "frame=env.render()  # ANSI text map with S,F,H,G\n",
        "print(frame)\n",
        "\n",
        "gamma=0.99          # discount factor\n",
        "theta=1e-8          # small threshold for convergence\n",
        "nS=env.observation_space.n  # number of states (16)\n",
        "nA=env.action_space.n       # number of actions (4)\n",
        "\n",
        "# 2) Define a fixed (random) policy π (NOT optimal)\n",
        "policy=np.ones((nS,nA))/nA   # uniform: each action has prob 1/nA in every state\n",
        "print(\"\\nRandom fixed policy (each row = probs over 4 actions):\")\n",
        "print(policy)\n",
        "\n",
        "# 3) Policy evaluation: compute V^π for this fixed policy (no max over actions)\n",
        "def policy_evaluation(env,policy,gamma=0.99,theta=1e-8,max_iter=1000):\n",
        "    P=env.unwrapped.P\n",
        "    nS=env.observation_space.n\n",
        "    nA=env.action_space.n\n",
        "    V=np.zeros(nS)          # start with V(s)=0 for all states\n",
        "    it=0\n",
        "    while it<max_iter:\n",
        "        delta=0.0\n",
        "        for s in range(nS):\n",
        "            v_old=V[s]\n",
        "            v_new=0.0\n",
        "            # Bellman expectation: sum_a π(a|s) * sum_{s'} P(s'|s,a)[r + γ V(s')]\n",
        "            for a,action_prob in enumerate(policy[s]):\n",
        "                if action_prob==0:continue\n",
        "                for prob,next_s,reward,done in P[s][a]:\n",
        "                    v_new+=action_prob*prob*(reward+gamma*V[next_s])\n",
        "            V[s]=v_new\n",
        "            delta=max(delta,abs(v_old-v_new))\n",
        "        it+=1\n",
        "        if delta<theta:break\n",
        "    print(f\"\\nPolicy evaluation converged in {it} iterations.\")\n",
        "    return V\n",
        "\n",
        "V_pi=policy_evaluation(env,policy,gamma,theta)\n",
        "print(\"\\nState values V^π (as 4x4 grid, under random policy):\")\n",
        "print(V_pi.reshape(4,4))\n",
        "\n",
        "# 4) Greedy policy improvement (one step) using V^π\n",
        "def q_from_v(env,V,s,gamma=0.99):\n",
        "    P=env.unwrapped.P\n",
        "    nA=env.action_space.n\n",
        "    q=np.zeros(nA)\n",
        "    for a in range(nA):\n",
        "        for prob,next_state,reward,done in P[s][a]:\n",
        "            q[a]+=prob*(reward+gamma*V[next_state])\n",
        "    return q\n",
        "\n",
        "def policy_improvement(env,V,discount_factor=0.99):\n",
        "    nS=env.observation_space.n\n",
        "    nA=env.action_space.n\n",
        "    new_policy=np.zeros((nS,nA))\n",
        "    for s in range(nS):\n",
        "        Q=q_from_v(env,V,s,discount_factor)   # Q(s,a) from V^π\n",
        "        best_action=np.argmax(Q)              # greedy: argmax_a Q(s,a)\n",
        "        new_policy[s]=np.eye(nA)[best_action] # one-hot row\n",
        "    return new_policy\n",
        "\n",
        "policy_improved=policy_improvement(env,V_pi,gamma)\n",
        "\n",
        "print(\"\\nOld random policy (π):\")\n",
        "print(policy)\n",
        "print(\"\\nGreedy improved policy (π') from V^π:\")\n",
        "print(policy_improved)\n",
        "\n",
        "# 5) Visualization helper (same as before)\n",
        "def plot(V,policy,col_ramp=1,dpi=175,draw_vals=True,title_suffix=\"\"):\n",
        "    plt.rcParams['figure.dpi']=dpi\n",
        "    plt.rcParams.update({'axes.edgecolor':(0.32,0.36,0.38)})\n",
        "    plt.rcParams.update({'font.size':6 if env.unwrapped.nrow==8 else 8})\n",
        "    plt.figure(figsize=(3,3))\n",
        "\n",
        "    desc=env.unwrapped.desc           # map layout: S,F,H,G\n",
        "    nrow,ncol=desc.shape\n",
        "    V_sq=V.reshape((nrow,ncol))       # reshape V into 4x4 grid\n",
        "\n",
        "    plt.imshow(V_sq,cmap='cool' if col_ramp else 'gray',alpha=0.7)\n",
        "    ax=plt.gca()\n",
        "\n",
        "    arrow_dict={0:'←',1:'↓',2:'→',3:'↑'}  # action arrows\n",
        "\n",
        "    # draw grid lines\n",
        "    for x in range(ncol+1):\n",
        "        ax.axvline(x-0.5,lw=0.5,color='black')\n",
        "    for y in range(nrow+1):\n",
        "        ax.axhline(y-0.5,lw=0.5,color='black')\n",
        "\n",
        "    # fill each cell with tile label, value, and arrow\n",
        "    for r in range(nrow):\n",
        "        for c in range(ncol):\n",
        "            s=r*ncol+c\n",
        "            val=V[s]\n",
        "            tile=desc[r,c].decode('utf-8')  # b'S' -> 'S'\n",
        "\n",
        "            if tile=='H':color='red'\n",
        "            elif tile=='G':color='green'\n",
        "            elif tile=='S':color='blue'\n",
        "            else:color='black'\n",
        "\n",
        "            plt.text(c,r,tile,ha='center',va='center',\n",
        "                     color=color,fontsize=10,fontweight='bold')\n",
        "\n",
        "            if draw_vals and tile not in ['H']:  # do not print value on holes\n",
        "                plt.text(c,r+0.3,f\"{val:.2f}\",ha='center',va='center',\n",
        "                         color='black',fontsize=6)\n",
        "\n",
        "            if policy is not None:\n",
        "                best_action=np.argmax(policy[s])\n",
        "                plt.text(c,r-0.25,arrow_dict[best_action],ha='center',va='center',\n",
        "                         color='purple',fontsize=12)\n",
        "\n",
        "    plt.title(f\"FrozenLake: V^π and policy {title_suffix}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# 6) Plot old and improved policies (with same V^π just for comparison of arrows)\n",
        "plot(V_pi,policy,draw_vals=True,title_suffix=\"(old random π)\")\n",
        "plot(V_pi,policy_improved,draw_vals=True,title_suffix=\"(greedy improved π')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B0h6b5Zxw8MZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}