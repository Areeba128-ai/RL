{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lImXx7bcsm3N"
      },
      "outputs": [],
      "source": [
        "### Code cell 0 ###\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)  # pretty printing of numpy arrays\n",
        "\n",
        "# Step 1 – Environment setup (FrozenLake)\n",
        "env = gym.make('FrozenLake-v1', is_slippery=True, render_mode='ansi')\n",
        "obs, info = env.reset()\n",
        "\n",
        "print(\"Initial State:\", obs)\n",
        "print(\"Observation space size:\", env.observation_space.n)  # 16 states\n",
        "print(\"Action space size:\", env.action_space.n)            # 4 actions\n",
        "\n",
        "# Transition model\n",
        "P = env.unwrapped.P\n",
        "reward_min = min({r for s in P for a in P[s] for (_, _, r, _) in P[s][a]})\n",
        "reward_max = max({r for s in P for a in P[s] for (_, _, r, _) in P[s][a]})\n",
        "print(\"Reward range:\", (reward_min, reward_max))\n",
        "\n",
        "print(env.render())\n",
        "\n",
        "# Step 3 – Value Iteration Algorithm\n",
        "def value_iteration(env, discount_factor=0.99, theta=1e-6, max_iterations=10000):\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "    P = env.unwrapped.P\n",
        "    V = np.zeros(nS)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        delta = 0\n",
        "        for s in range(nS):\n",
        "            q_sa = np.zeros(nA)\n",
        "            for a in range(nA):\n",
        "                for prob, next_state, reward, done in P[s][a]:\n",
        "                    q_sa[a] += prob * (reward + discount_factor * V[next_state])\n",
        "            new_v = np.max(q_sa)\n",
        "            delta = max(delta, abs(new_v - V[s]))\n",
        "            V[s] = new_v\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    policy = extract_policy_from_v(env, V, discount_factor)\n",
        "    return V, policy, i + 1\n",
        "\n",
        "\n",
        "# Step 4 – Extract policy from V\n",
        "def extract_policy_from_v(env, V, discount_factor=0.99):\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "    P = env.unwrapped.P\n",
        "    policy = np.zeros((nS, nA))\n",
        "\n",
        "    for s in range(nS):\n",
        "        q_sa = np.zeros(nA)\n",
        "        for a in range(nA):\n",
        "            for prob, next_state, reward, done in P[s][a]:\n",
        "                q_sa[a] += prob * (reward + discount_factor * V[next_state])\n",
        "        best_action = np.argmax(q_sa)\n",
        "        policy[s] = np.eye(nA)[best_action]\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "# Evaluate a learned policy by running episodes\n",
        "def evaluate_policy(env, policy, n_episodes=1000):\n",
        "    success = 0\n",
        "    for _ in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = np.argmax(policy[obs])\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            # in FrozenLake, reaching the goal gives reward=1 at the terminal step\n",
        "            if done and reward > 0:\n",
        "                success += 1\n",
        "    return success / n_episodes\n",
        "\n",
        "\n",
        "# Step 5 – Visualization: Value Function\n",
        "def plot_values(env, V, gamma_label=\"\"):\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    plt.plot(V, marker='o')\n",
        "    title = \"Value Function for FrozenLake-v1\"\n",
        "    if gamma_label:\n",
        "        title += f\" (gamma={gamma_label})\"\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"State (0–15)\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Visualization: Policy\n",
        "def plot_policy(env, policy, gamma_label=\"\"):\n",
        "    nS = env.observation_space.n\n",
        "    actions = np.argmax(policy, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    plt.bar(np.arange(nS), actions)\n",
        "    title = \"Greedy Policy (Best Action per State)\"\n",
        "    if gamma_label:\n",
        "        title += f\" (gamma={gamma_label})\"\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"State Index (0–15)\")\n",
        "    plt.ylabel(\"Action (0=Left,1=Down,2=Right,3=Up)\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Step 6 – Run Value Iteration for multiple discount factors\n",
        "if __name__ == \"__main__\":\n",
        "    gammas = [0.9, 0.99, 0.6]\n",
        "    all_V = {}\n",
        "    all_iters = []\n",
        "    success_rates = []\n",
        "\n",
        "    for gamma in gammas:\n",
        "        print(f\"\\n=== Running Value Iteration with gamma = {gamma} ===\")\n",
        "        V_opt, policy_opt, iterations = value_iteration(env, discount_factor=gamma)\n",
        "        all_V[gamma] = V_opt\n",
        "        all_iters.append(iterations)\n",
        "\n",
        "        print(f\"Converged in {iterations} iterations for gamma = {gamma}\")\n",
        "\n",
        "        # evaluate learned policy\n",
        "        rate = evaluate_policy(env, policy_opt, n_episodes=1000)\n",
        "        success_rates.append(rate)\n",
        "        print(f\"Success Rate for gamma={gamma}: {rate * 100:.2f}%\")\n",
        "\n",
        "        plot_values(env, V_opt, gamma_label=str(gamma))\n",
        "        plot_policy(env, policy_opt, gamma_label=str(gamma))\n",
        "\n",
        "    # Compare value functions for all gammas\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    for gamma in gammas:\n",
        "        plt.plot(all_V[gamma], label=f\"gamma={gamma}\")\n",
        "    plt.title(\"Comparison of Value Functions for Different Discount Factors (FrozenLake)\")\n",
        "    plt.xlabel(\"State (0–15)\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Compare convergence speed\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar([str(g) for g in gammas], all_iters)\n",
        "    plt.title(\"Convergence Speed vs Discount Factor (FrozenLake)\")\n",
        "    plt.xlabel(\"Gamma\")\n",
        "    plt.ylabel(\"Iterations to Converge\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot success rate vs gamma\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar([str(g) for g in gammas], [r * 100 for r in success_rates])\n",
        "    plt.title(\"Policy Success Rate vs Discount Factor (FrozenLake)\")\n",
        "    plt.xlabel(\"Gamma\")\n",
        "    plt.ylabel(\"Success Rate (%)\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "### Code cell 1 ###\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VyQAaRHcLZAi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}