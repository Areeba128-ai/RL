{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lImXx7bcsm3N"
      },
      "outputs": [],
      "source": [
        "#  In this Uses random V (just for demonstration), not calucyltaed Computes Q(s,a) from that V,\n",
        "#  Does greedy policy improvement to get a deterministic policy,\n",
        "#  Plots both V(s) as numbers and π(s) as arrows.\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(precision=3,suppress=True)  # show 3 decimal, no scientific format\n",
        "\n",
        "# Create FrozenLake environment (slippery)\n",
        "env=gym.make('FrozenLake-v1',is_slippery=True,render_mode='ansi')  # 4x4 lake, stochastic\n",
        "\n",
        "obs,info=env.reset()\n",
        "print(\"Initial State:\",obs)\n",
        "print(\"Action Space:\",env.action_space)           # Discrete(4) -> 0=L,1=D,2=R,3=U\n",
        "print(\"Observation Space:\",env.observation_space) # Discrete(16) -> 16 states in 4x4 grid\n",
        "print(\"Grid shape (rows, cols):\",(4,4))\n",
        "\n",
        "# Reward range from transition model\n",
        "P=env.unwrapped.P   # full transition dictionary P[s][a] -> list of (prob,next_state,reward,done)\n",
        "reward_min=min({r for s in P for a in P[s] for (_,_,r,_) in P[s][a]})\n",
        "reward_max=max({r for s in P for a in P[s] for (_,_,r,_) in P[s][a]})\n",
        "print(\"Reward range:\",(reward_min,reward_max))\n",
        "\n",
        "# Render text version of the lake\n",
        "frame=env.render()  # shows S, F, H, G as text\n",
        "print(frame)\n",
        "\n",
        "#  Compute Q-values from a given value function V\n",
        "def q_from_v(env,V,s,gamma=1.0):\n",
        "    P=env.unwrapped.P\n",
        "    nA=env.action_space.n\n",
        "    q=np.zeros(nA)                 # q[a] will store Q(s,a)\n",
        "    for a in range(nA):            # loop over all actions\n",
        "        for prob,next_state,reward,done in P[s][a]:  # sum over all possible next states\n",
        "            q[a]+=prob*(reward+gamma*V[next_state])  # Bellman expectation formula\n",
        "    return q\n",
        "\n",
        "#  Greedy policy improvement from a given V\n",
        "def policy_improvement(env,V,discount_factor=1.0):\n",
        "    nS=env.observation_space.n\n",
        "    nA=env.action_space.n\n",
        "    policy=np.zeros((nS,nA))       # each row = probability over actions in that state\n",
        "    for s in range(nS):\n",
        "        Q=q_from_v(env,V,s,discount_factor)  # compute Q(s,a) for all a\n",
        "        best_action=np.argmax(Q)             # pick action with max Q\n",
        "        policy[s]=np.eye(nA)[best_action]    # one-hot: 1.0 on best action, 0 for others\n",
        "    return policy\n",
        "\n",
        "# Visualization function\n",
        "def plot(V,policy,discount_factor=1.0,draw_vals=True):\n",
        "    nrow=env.unwrapped.nrow  # 4\n",
        "    ncol=env.unwrapped.ncol  # 4\n",
        "    arrow_symbols={0:'←',1:'↓',2:'→',3:'↑'}  # mapping action index to arrow\n",
        "\n",
        "    grid=np.reshape(V,(nrow,ncol))  # reshape V into 4x4 grid\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(grid,cmap='cool',interpolation='none')  # color map based on V\n",
        "\n",
        "    for s in range(nrow*ncol):\n",
        "        row,col=divmod(s,ncol)        # convert state index -> (row,col)\n",
        "        best_action=np.argmax(policy[s])\n",
        "\n",
        "        if draw_vals:\n",
        "            # print value inside cell\n",
        "            plt.text(col,row,f'{V[s]:.2f}',ha='center',va='center',\n",
        "                     color='white',fontsize=10)\n",
        "        else:\n",
        "            # print arrow for best action\n",
        "            plt.text(col,row,arrow_symbols[best_action],ha='center',va='center',\n",
        "                     color='white',fontsize=14)\n",
        "\n",
        "    plt.title(\"Value Function\" if draw_vals else \"Policy\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Run policy improvement and visualize\n",
        "\n",
        "# Example value function (random initialization)\n",
        "V=np.random.rand(env.observation_space.n)   # random V, not learned, just to test code\n",
        "\n",
        "policy=policy_improvement(env,V)            # compute greedy policy from this V\n",
        "\n",
        "# Plot Value Function (numbers in grid)\n",
        "plot(V,policy,1.0,draw_vals=True)\n",
        "\n",
        "# Plot Policy (arrows only)\n",
        "plot(V,policy,1.0,draw_vals=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UxeU_UOiso5Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}