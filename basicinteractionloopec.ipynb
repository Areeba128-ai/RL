{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE5EE-H-rjky"
      },
      "outputs": [],
      "source": [
        "def EnvironmentSetup():\n",
        "    import gymnasium as gym\n",
        "    import numpy as np\n",
        "    if not hasattr(np,\"bool8\"):np.bool8=np.bool_\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    env=gym.make('CliffWalking-v0')\n",
        "    state,info=env.reset()\n",
        "\n",
        "    print(\"=== CliffWalking-v0 ===\")\n",
        "    print(\"Initial State:\",state)\n",
        "    print(\"Action Space:\",env.action_space)          # Discrete(4)\n",
        "    print(\"Observation Space:\",env.observation_space) # Discrete(48)\n",
        "    print(\"Actions: 0=Up, 1=Right, 2=Down, 3=Left\")\n",
        "    print(\"Reward Scheme: step=-1, cliff=-100 (done), goal=-1 (done)\")\n",
        "\n",
        "    return env,state\n",
        "\n",
        "env,state=EnvironmentSetup()\n",
        "\n",
        "def InteractionLoop(env,num_episodes=5):\n",
        "    for episode in range(num_episodes):\n",
        "        state,info=env.reset()\n",
        "        terminated=False\n",
        "        total_reward=0\n",
        "        step_count=0\n",
        "        while not terminated:\n",
        "            action=env.action_space.sample()\n",
        "            next_state,reward,terminated,truncated,info=env.step(action)\n",
        "            terminated=terminated or truncated   # merge flags\n",
        "            total_reward+=reward\n",
        "            print(f\"Step {step_count}: State={state}, Action={action}, Reward={reward}, Next State={next_state}, Terminated={terminated}\")\n",
        "            state=next_state\n",
        "            step_count+=1\n",
        "        print(f\"Episode {episode+1} ended with total reward: {total_reward}\\n\")\n",
        "\n",
        "InteractionLoop(env,5)\n",
        "\n",
        "############\n",
        "def VisualizePathComparison(env,rows=4,cols=12,num_episodes=3):\n",
        "    import numpy as np\n",
        "\n",
        "    def visualize_path(path,rows=4,cols=12):\n",
        "        grid=np.full((rows,cols),'-')\n",
        "        visit_count=np.zeros((rows,cols),dtype=int)\n",
        "        for step,state in enumerate(path):\n",
        "            r,c=divmod(state,cols)\n",
        "            grid[r,c]=str(step)\n",
        "            visit_count[r,c]+=1\n",
        "        print(\"Path Grid (Step Order):\")\n",
        "        print(grid)\n",
        "        print(\"\\nVisit Count Grid:\")\n",
        "        print(visit_count)\n",
        "        most_visited=np.unravel_index(np.argmax(visit_count),(rows,cols))\n",
        "        print(f\"\\nMost visited cell: {most_visited} visited {visit_count[most_visited]} times\")\n",
        "\n",
        "    paths=[]\n",
        "    steps_list=[]\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        state,info=env.reset()\n",
        "        terminated=False\n",
        "        path=[state]\n",
        "        step_count=0\n",
        "        while not terminated:\n",
        "            action=env.action_space.sample()\n",
        "            state,reward,terminated,truncated,info=env.step(action)\n",
        "            terminated=terminated or truncated\n",
        "            path.append(state)\n",
        "            step_count+=1\n",
        "        print(f\"\\nEpisode {ep+1} finished in {step_count} steps\")\n",
        "        visualize_path(path,rows,cols)\n",
        "        paths.append(path)\n",
        "        steps_list.append(step_count)\n",
        "\n",
        "    print(\"\\n--- Path Efficiency Comparison ---\")\n",
        "    for i,steps in enumerate(steps_list):\n",
        "        print(f\"Episode {i+1}: {steps} steps\")\n",
        "    best_ep=np.argmin(steps_list)\n",
        "    print(f\"\\nMost efficient path: Episode {best_ep+1} with {steps_list[best_ep]} steps\")\n",
        "\n",
        "VisualizePathComparison(env,rows=4,cols=12,num_episodes=3)\n",
        "#####\n",
        "\n",
        "def VisualizePathRun(env,rows=4,cols=12):\n",
        "    import numpy as np\n",
        "    def visualize_path(path,rows=4,cols=12):\n",
        "        grid=np.full((rows,cols),'-')\n",
        "        for step,state in enumerate(path):\n",
        "            r,c=divmod(state,cols)\n",
        "            grid[r,c]=str(step)\n",
        "        print(grid)\n",
        "\n",
        "    state,info=env.reset()\n",
        "    terminated=False\n",
        "    path=[state]\n",
        "    while not terminated:\n",
        "        action=env.action_space.sample()\n",
        "        next_state,reward,terminated,truncated,info=env.step(action)\n",
        "        terminated=terminated or truncated   # merge flags\n",
        "        state=next_state\n",
        "        path.append(state)\n",
        "    visualize_path(path,rows,cols)\n",
        "\n",
        "path_return=VisualizePathRun(env)\n",
        "\n",
        "def TrackCumulativeRewards(env,n_episodes=10,plot=True):\n",
        "    import matplotlib.pyplot as plt\n",
        "    rewards=[]\n",
        "    for episode in range(n_episodes):\n",
        "        state,info=env.reset()\n",
        "        terminated=False\n",
        "        total_reward=0\n",
        "        while not terminated:\n",
        "            action=env.action_space.sample()\n",
        "            state,reward,terminated,truncated,info=env.step(action)\n",
        "            terminated=terminated or truncated   # merge flags\n",
        "            total_reward+=reward\n",
        "        rewards.append(total_reward)\n",
        "    if plot:\n",
        "        plt.figure()\n",
        "        plt.plot(rewards)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Total Reward\")\n",
        "        plt.title(\"Random Policy Reward per Episode (CliffWalking)\")\n",
        "        plt.show()\n",
        "    return rewards\n",
        "\n",
        "rewards=TrackCumulativeRewards(env,n_episodes=10,plot=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0q7V1GvtsD90"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}