{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd1WheIe7t3n"
      },
      "outputs": [],
      "source": [
        "### Code cell 0 ###\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True) #for pretty printing of nupyarray, suppes=true means do not show 1.0e values\n",
        "\n",
        "env=gym.make('FrozenLake-v1',render_mode='ansi') #ansi=text form of girl insated of grahival window\n",
        "state=env.reset()\n",
        "\n",
        "print(\"Initial State:\",state)\n",
        "print(\"Action Space:\",env.action_space)\n",
        "print(\"Observation Space:\",env.observation_space)\n",
        "print(\"Grid shape (rows, cols):\",(4,4))\n",
        "print(\"Reward range:\",(min({r for s in env.unwrapped.P\n",
        "                            for a in env.unwrapped.P[s]\n",
        "                            for (_,_,r,_) in env.unwrapped.P[s][a]}), # _ means ignorieng other just care about reward\n",
        "                       max({r for s in env.unwrapped.P\n",
        "                            for a in env.unwrapped.P[s]\n",
        "                            for (_,_,r,_) in env.unwrapped.P[s][a]})))\n",
        "#here env.unwrpapped in trabsition model for each state and actoion a, P[s][a] is a list if tuple. whch is (prob,next_sate,reward,done) collect rewards and prints min, max.\n",
        "\n",
        "obs, info = env.reset(seed=42) #for same behvaior\n",
        "frame = env.render()  # returns ANSI text since render_mode='ansi'\n",
        "print(frame)\n",
        "\n",
        "gamma=0.99\n",
        "theta=1e-8\n",
        "\n",
        "P=env.unwrapped.P #saving transitaon model in P\n",
        "nS=env.observation_space.n    # number of states\n",
        "nA=env.action_space.n         # number of actions\n",
        "\n",
        "\n",
        "def plot(V,policy,col_ramp=1,dpi=175,draw_vals=False): #using coll clor map, dot pr inch sharper image\n",
        "    #    Visualize FrozenLake state values V and policy arrows.\n",
        "\n",
        "    plt.rcParams['figure.dpi']=dpi #global value setting\n",
        "    plt.rcParams.update({'axes.edgecolor':(0.32,0.36,0.38)})\n",
        "    plt.rcParams.update({'font.size':6 if env.unwrapped.nrow==8 else 8}) # 8 fomt sizee styling optns\n",
        "    plt.figure(figsize=(3,3))\n",
        "\n",
        "    desc=env.unwrapped.desc\n",
        "    nrow,ncol=desc.shape # grid size\n",
        "    V_sq=V.reshape((nrow,ncol)) #rehspae value into same grid size eree 4*4 4x4\n",
        "\n",
        "    plt.imshow(V_sq,cmap='cool' if col_ramp else 'gray',alpha=0.7) #imshow for color intenisty. high value high intensity\n",
        "    ax=plt.gca()\n",
        "\n",
        "    arrow_dict={0:'←',1:'↓',2:'→',3:'↑'}\n",
        "\n",
        "    for x in range(ncol+1):  # for grid lines\n",
        "        ax.axvline(x-0.5,lw=0.5,color='black')\n",
        "    for y in range(nrow+1):\n",
        "        ax.axhline(y-0.5,lw=0.5,color='black')\n",
        "\n",
        "    for r in range(nrow): # for each cell convert to state using row,col\n",
        "        for c in range(ncol):\n",
        "            s=r*ncol+c\n",
        "            val=V[s]\n",
        "            tile=desc[r,c].decode('utf-8')  # turn b'S' into 'S'.  2-d array to normal string\n",
        "\n",
        "            if tile=='H':color='red' #pick colro form title letter\n",
        "            elif tile=='G':color='green'\n",
        "            elif tile=='S':color='blue'\n",
        "            else:color='black'\n",
        "\n",
        "            plt.text(c,r,tile,ha='center',va='center',color=color,fontsize=10,fontweight='bold')\n",
        "\n",
        "            if draw_vals and tile not in ['H']: # prit value if not hole\n",
        "                plt.text(c,r+0.3,f\"{val:.2f}\",ha='center',va='center',color='black',fontsize=6)\n",
        "\n",
        "            if policy is not None: # find best action from policy row and draw arrow in that directin\n",
        "                best_action=np.argmax(policy[s])\n",
        "                plt.text(c,r-0.25,arrow_dict[best_action],ha='center',va='center',color='purple',fontsize=12)\n",
        "\n",
        "    plt.title(\"FrozenLake: Policy and State Values\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# random policy initialisation\n",
        "policy=np.ones((nS,nA))/nA      # random policy: each action 1/nA\n",
        "\n",
        "\n",
        "def policy_evaluation(env,policy,discount_factor=1.0,theta=1e-9,draw=False):\n",
        "    nS=env.observation_space.n\n",
        "    nA=env.action_space.n\n",
        "    V=np.zeros(nS)              # initial V(s)=0 for all states\n",
        "    P=env.unwrapped.P           # transition dynamics: P[s][a] -> list of (prob,next_state,reward,done)\n",
        "\n",
        "    while True:\n",
        "        delta=0.0\n",
        "        for s in range(nS):\n",
        "            v=0.0\n",
        "            for a,action_prob in enumerate(policy[s]):\n",
        "                if action_prob==0:continue\n",
        "                for prob,next_state,reward,done in P[s][a]:\n",
        "                    v+=action_prob*prob*(reward+discount_factor*V[next_state])\n",
        "            delta=max(delta,abs(V[s]-v))\n",
        "            V[s]=v\n",
        "        if delta<theta:break\n",
        "\n",
        "    if draw:\n",
        "        side=int(np.sqrt(nS))   # works for 4x4 FrozenLake\n",
        "        print(\"Value function after policy evaluation:\")\n",
        "        print(V.reshape((side,side)))\n",
        "    return V\n",
        "\n",
        "\n",
        "V=policy_evaluation(env,policy,discount_factor=gamma,theta=theta,draw=True)\n",
        "plot(V,policy,draw_vals=True)\n",
        "\n",
        "### Code cell 1 ###\n"
      ]
    }
  ]
}