{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd1WheIe7t3n"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "env=gym.make('FrozenLake-v1',render_mode='ansi')\n",
        "state=env.reset()\n",
        "\n",
        "print(\"Initial State:\",state)\n",
        "print(\"Action Space:\",env.action_space)\n",
        "print(\"Observation Space:\",env.observation_space)\n",
        "print(\"Grid shape (rows, cols):\",(5,5))\n",
        "print(\"Reward range:\",(min({r for s in env.unwrapped.P for a in env.unwrapped.P[s] for (_,_,r,_) in env.unwrapped.P[s][a]}),max({r for s in env.unwrapped.P for a in env.unwrapped.P[s] for (_,_,r,_) in env.unwrapped.P[s][a]})))\n",
        "\n",
        "obs, info = env.reset(seed=42)\n",
        "frame = env.render()  # returns ANSI text since render_mode='ansi'\n",
        "print(frame)\n",
        "\n",
        "gamma=0.99\n",
        "theta=1e-8\n",
        "V=np.zeros(env.observation_space.n)\n",
        "\n",
        "P=env.unwrapped.P\n",
        "print(P)\n",
        "nS=env.observation_space.n    # number of states\n",
        "nA=env.action_space.n\n",
        "\n",
        "def value_iteration(env,gamma,theta=1e-4,max_iter=1000):\n",
        "    V=np.zeros(nS)                # start with V(s)=0 for all states\n",
        "    it=0\n",
        "    while it<max_iter:\n",
        "        delta=0.0\n",
        "        for s in range(nS):\n",
        "            v=V[s]\n",
        "            q_sa=[]\n",
        "            for a in range(nA):\n",
        "                q=0.0\n",
        "                for prob,next_s,reward,done in P[s][a]:\n",
        "                    q+=prob*(reward+gamma*V[next_s])\n",
        "                q_sa.append(q)\n",
        "            V[s]=max(q_sa)\n",
        "            delta=max(delta,abs(v-V[s]))\n",
        "        it+=1\n",
        "        if delta<theta:break\n",
        "    return V\n",
        "\n",
        "V_opt=value_iteration(env,gamma,theta)\n",
        "print(\"Optimal state values V*:\")\n",
        "print(\"V* reshaped as 4x4 grid:\")\n",
        "print(V_opt.reshape(4,4))\n",
        "\n",
        "def optimal_policy(env,V,gamma):\n",
        "    P=env.unwrapped.P\n",
        "    nS=env.observation_space.n\n",
        "    nA=env.action_space.n\n",
        "    policy=np.zeros((nS,nA))\n",
        "    for s in range(nS):\n",
        "        q_sa=np.zeros(nA)\n",
        "        for a in range(nA):\n",
        "            for prob,next_state,reward,done in P[s][a]:\n",
        "                q_sa[a]+=prob*(reward+gamma*V[next_state])\n",
        "        best_action=np.argmax(q_sa)\n",
        "        policy[s][best_action]=1.0\n",
        "    return policy\n",
        "V_opt=value_iteration(env,gamma,theta)\n",
        "policy=optimal_policy(env,V_opt,gamma)\n",
        "print(\"Optimal policy (one-hot over actions):\")\n",
        "print(policy)\n",
        "\n"
      ]
    }
  ]
}